%However, care have been taken to allow for future extension generative model similar to the one used by \citet{richardson2016generalized}, further described in section \ref{section:languange_models}. The main obstacle for applying \citeauthor{richardson2016generalized}:s method directly on a model for abstract dependency trees is that the order of traversal is defined using linear word order of a sentence, an attribute that is normally not defined in the abstract domain, as it is a very language dependent attribute. This could be partly mitigated by defining traversal order in terms of the dependency labels of each edge, but problems arise again when there are two or more siblings with the same dependency label in a tree. In this work the issue is resolved by only considering $t$-treelets of size $l=2$, meaning that there will only be one $t$-treelet for each node, namely the one with the node itself and its parent simplifying the model to a simple syntactic bigram model. It thus becomes unnecessary to define a complete ordering of the tree and we can rely only on the partial ordering imposed naturally by the ancestor relation in the dependency tree. However, it is worth to note that should the problem of well defined traversal of the tree be resolved the method outlined could be adapted to models based on more general types of $t$-treelets as well.


%\section{Re-ranking and application to AST disambiguation}
%One of the main applications considered in this thesis is to use the probabilistic model of abstract dependency trees to rerank parses produced by the current GF parser in order to increase the precision in disambiguating correct abstract syntax trees. This is done by utilizing the capability of transforming abstract syntax to abstract dependency syntax using GF2UD, and scoring the k-best parses produced by the GF parser with the abstract dependency syntax probabilistic model, ultimately choosing the one with the highest score.

%The tree disambiguation model proposed works by assigning a probability score to each possible abstract syntax tree. In this section the method for defining such probability scores is further described.\\
%The main problem in estimating scores for trees is sparseness of data and the massive size of the space of possible trees, we will simply not be able to observe every possible AST enough times to be able to predict frequency of occurrence for arbitrary AST:s. To work around this problem we will need to make certain assumptions of the distribution of AST:s in order to reduce the amount of parameters that needs to be estimated. One such approach is the context free probability model, which states that the occurence of each constituent part of a tree is independent from occurence of all other constituent parts of the tree. While such an assumption means that it is only necessary to estimate the frequency of each individual constituent part it also means that much information present in available data is likely discarded, and certain patterns in the data is not accounted for. 


%The method used to assign probabilities to AST:s can be viewed as a lexical context based model. That we call it lexical context based is because the model assigns probabilities to every lexical item in the AST, and those probabilities are dependent on a syntactic neighborhood of the item. In the process of converting an AST to an UD tree with the method described in \citep{kolachina2016gf2ud} we obtain an abstract dependency tree, that is a dependency tree mapped with UD dependency labels over the GF terminal functions. The syntactic neighborhoods used to condition probabilities on are defined in terms of the abstract UD dependency tree corresponding to the AST. For example we can condition the probability of each lexical item on its parent in the dependency tree. This approach although very mildly context based in the UD realm will capture potentially quite long range context dependencies in the AST realm. The advantages of using this approach is that this allows the use of the many high quality UD-parsers available to generate training data to estimate parameters for the model, through the use of unsupervised methods like the EM algorithm to overcome the ambiguity resulting in going from a parsed UD tree to an abstract UD tree.

\section{Defining a probabilistic model for ADT:s}
A way to define a probabilistic model of abstract syntax trees is to first define a deterministic way to traverse the nodes of a tree in order to be able to index them in a well defined way. Having such an indexing scheme means that we can represent each dependency tree as a set of labeled nodes $T=\{N_j\}_{j=0}^n$ and a mapping $H_T$ such that $H_T(N_j)$ is the head of $N_j$ or a dummy root node if $N_j$ has no head. By restricting the traversal order to only consider orderings that ensures that the head of a node always precedes the node itself, we ensure that the nodes with an index smaller than or equal to a certain number $n$ will always be a valid subtree. We denote these nodes $T_n={N_i : i\leq n}$, now the condition put on the traversal order ensures that $N\in T_n \implies H_T(N)\in T_n$ which means that we can see the pair $T_n, H_{T_n}$ as representing that subtree, where $H_{T_n}$ is simply $H_T$ with the domain restricted to $T_n$. 
A generative probabilistic model for a tree $T,H_T$ can now be constructed by viewing the subtree $T_{i-1}, H_{T_{i-1}}$ as the \emph{history} of node $N_i$ allowing a generative model where the probability of seeing a certain tree can be factorized into the contitional probabilities of generating every node conditioned on the history of the nodes. Such a model can be written in the following way with the help of the chain rule of conditional probability.
\begin{equation}
\begin{split}
    P(T, H_T) & = \prod^{|T|+1}_{i=1} P(T_i, H_{T_i}|T_{i-1}, H_{T_{i-1}})\\
    & = \prod P(N_i, H_{T_i}(N_i)|T_{i-1}, H_{T_{i-1}})
\end{split}
\end{equation}
Where the probabilities after the second equality should be interpreted as the probability that the node with index $i$ is $N_i$ and that it is child to $H_{T_i}(N_i)$ given the complete subtree preceding that index. Technically one needs to look out for the edge cases $i=|T|+1$ and $i=0$, these sould be interpreted as the probabily of a tree starting with $N_1$ (that is having $N_1$ as its root node) and the probability that there are no nodes succeeding $N_{|T|}$ respectively. It is here easy to draw parallels to generative language models on linear texts, by identifying all 
\todo{Continue rewrite of this section}
As estimating probabilities for this type of model is completely unfeasible as we will never observe all possible kinds of histories in any data set, we can simplify the model by taking inspiration from n-gram models in linear text and only condition the probability of each node on parts of its history, as well as making probabilities location invariant. %However instead of only conditioning of the immediately preceding node or nodes we can in the tree case take advantage of the tree structure of the tree and only condition on the.
%$P(N_i, H_{T_i}(N_i)|T_{i-1}, H_{T_{i-1}})=P(N, H_{T_i}(N_i)|fT_{i-1}, H_{T_{i-1}})$
%We have factorized the probability of the tree into the probabilities of every tuple being a member of the tree conditioned on the event that all tuples with an index lower than that tuple is present in the tree. In order to mitigate the effect of sparsity of data we can now approximate these conditional probabilities with the probability conditioned on only a subset of the tuples preceding each tuple. For example if we define the traversal order of the indexing such that the parent of a node always occurs before its children, we can make the following approximation:
\section{Old}
\begin{equation*}
P(N_i\in T | \{N_j\}_{j=0}^{i-1}\subseteq T, |T|=n) \approx 
P(N_i\in T | f_T(N_i) \subseteq T, |T|=n) ,
\end{equation*}
where $f_T$ is a mapping from a node in the tree to a subset of the nodes with an index lower than that node. This is the same thing as saying that we assume that the occurence of node $N_j$ in the tree and the occurence of any of the nodes not in $f(N_i)$ is independent.
We can further approximate these probabilities by assuming independence between the label of a node, the value of the index of that node and the indicies of the nodes in $f(N_i)$ and their parents, which would mean that  
\begin{equation*}
P(N_i\in T | N_{H_j} \in T, |T|=n) = g(L_i, \text{Label} (N):N\in f_T(N_i), |T|),
\end{equation*}
where $g$ only depends of the node labels of the node $N_i$ and the nodes in the set $f(N_i)$, which gives us the tree probabilities: 

\begin{equation*}
    P(T=\{N_j\}_{j=0}^n) \approx P(N_0\in T) \prod_{i=1}^{n}g(L_i, \text{Label} (N):N\in f_T(N_i), |T|),
\end{equation*}
where $N_0$ will be the root node. The function $g$ can now be estimated from available data such as treebanks by for every node $N$ in the data set calculate $f_T(N)$. The way this estimation can be done is further described in section X. 

\subsection{Using GF2UD to Define Context}
This function $f_T$ can be seen as the function deciding what kind of context the model will take into account. For example it can be noted that by choosing $f_T$ to be the constant mapping to the empty set the model will be reduced to a completely context free model. If we take more nodes into account the model will be able to take more context into account when assigning a probability score for a tree, but this also comes at a cost. The more nodes taken into account by the context function, the harder it will be to estimate. It is therefore very important to define the context function in a clever way in order to only capture the contextual information that are the most useful to determine weather a tree is probable or not.

A way to chose the context function $f_T$ for the model is through the use of the GF-function tagged abstract UD dependency trees generated by the mapping from GF to UD described in \citep{kolachina2016gf2ud}. This gives us a new tree structure over only the terminal nodes of the AST, and we can now define the context function $f_T$ for the terminal symbols in terms of the abstract dependency tree in the same way as described in the previous section by defining an order of traversal for indexing of the dependency tree. If we define the order of traversal for indexing of the AST in a way that preserves the ordering of the abstract dependency tree, we can even use such a model for the terminal nodes together with a model for the non terminal nodes.
