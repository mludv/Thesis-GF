\chapter{Results}
\label{chapter:results}
The problem with unsupervised methods like ours is that there is no good standardized tests to use in order to evaluate performance. Since the main goal is to define a probability model for disambiguating abstract syntax trees, the obvious evaluation metric would be how well it ranks the trees generated by the GF parser. To do this, we defined a few example sentences with known ambiguous meanings. Another evaluation metric used is sense disambiguation. The advantage in looking at this subproblem is that we can compare ourselves to other known methods. Here, we utilize the standard measurements precision, recall and F1: 
\begin{align*}
 \textbf{Precision} &= \frac{TP}{TP+FP} \\[1em]
 \textbf{Recall} &= \frac{TP}{TP+TN} \\[1em]
 \textbf{F1} &= \frac{2\cdot \textbf{Precision} \cdot \textbf{Recall}}{\textbf{Precision} + \textbf{Recall}}
\end{align*}

For this task, two datasets are used. One is ``trainomatic'' created by \citet{pasini2017trainomatic}, an automatically generated dataset with high quality sensed tagged data. Another is the Semeval 2015 disambiguation task \citep{moro2015semeval}, a word sense disambiguation task where all words in a sentence need to be disambiguated.

\section{Dictionaries}
As our model is dependent on the underlying \textbf{knowledge graph}, or dictionary, we made experiments using three different variants:

\begin{enumerate}
    \item Current GF dictionary
    \item Wordnet
    \item Merged Wordnet
\end{enumerate}
Currently, it's only the GF dictionary that is compatible with doing experiments with the GF parser, but for the words disambiguation evaluations we also did experiments on Wordnet and the merged Wordnet created by us. This merged Wordnet was created as described in section \ref{section:cluster}, some statistics is shown in table \ref{tab:cluster_stats}, and an example of the merged synset is shown in table \ref{tab:cluster}. 


\begin{table}[]
\centering
\begin{tabular}{llrr}
\textbf{Dictionary} & \textbf{Language} & \textbf{Lemmas} & \textbf{Ambiguous lemmas}\\
GF dictionary & English & 64,710 & 8.1\% \\
GF dictionary & Finnish & 40,907 & 35.5\% \\
GF dictionary & Bulgarian & 31,078 & 25.8\% \\
GF dictionary & Swedish & 30,196 & 21.2\% \\
GF dictionary & French & 23,901 & 23.5\% \\
Wordnet & English & 156,889 & 16.9\% \\
Wordnet & Finnish & 122,961 & 16.7\% \\
Wordnet & French  & 57,252 & 23.6\% \\
Wordnet & Bulgarian & 6,699 & 22.7\% \\
Wordnet & Swedish & 5,980 & 16.8\% \\
\end{tabular}

\caption{Statistics about Wordnet and the GF dictionary. Ambigiuous lemmas have more than one meaning.}
\label{tab:cluster_stats}
\end{table}


\begin{table}[]
\centering
\begin{tabular}{lr}
Average number of merged synsets per cluster: &$6.3$ \\
Number of clusters: & $5302$ \\
Number of synset merged: & $33402$ \\
\end{tabular}

\caption{Statistics about the merged Wordnet.}
\label{tab:cluster_stats}
\end{table}


\begin{table}[]
\centering
\begin{tabular}{c|ccc}
\textbf{Cluster} & spiritual\_leader.n.01 & investigation.n.02 & sketch.n.01 \\
\hline
\textbf{Synsets} & rabbi.n.01 & inquest.n.01 & draft.n.03 \\
& cantor.n.02 & inquiry.n.03 & vignette.n.03 \\
& patriarch.n.01 & tabulation.n.02 & \\
& catholicos.n.01 & wiretap.n.01 & \\
& evangelist.n.02 & empiricism.n.02 & 
\vspace{0.5cm} 
\\
\textbf{Cluster} & seafood.n.01 & bank.n.01 & \\
\hline
\textbf{Synsets} & octopus.n.01 & riverbank.n.01 & \\
& coral.n.03 & waterside.n.01 & \\
& whelk.n.01 & & \\
& roe.n.01 & & \\
& caviar.n.01 & & \\
& prawn.n.01 & & \\
& seafood.n.01 & & \\
\end{tabular}

\caption{A few example of the synsets merged together in our `cluster' model.}
\label{tab:cluster}
\end{table}


\section{Training}
\label{section:training}

We trained our models using parsed data from English, Swedish, Finnish, French, and Bulgarian, in total over $10^9$ sentences. We performed the training for each model three times: First, using all five languages, second, we removed English from the training data and trained the model using the other four languages, and third, we only trained the model using English data.

\section{GF Parser}
In order to develop a small test set of sentences for qualitative evaluation, we utilized the GF gold tree database which contains a small number of trees tagged correctly. Using these, we constructed a list of simple sentences where the GF parser struggled. One such sentence is ``I work at the bank'' where the GF parser have troubles disambiguating between ``work'' in the sense of laboring and in the sense of functioning. The model was a stupid backoff model as described in section \ref{sec:models}, trained on all seven languages. A summery of the results are available in table \ref{tab:gfparser} and for the full list of sentences, see appendix \ref{apx:sentences}

The top parsing results from the sentence ``he works at the bank'' can be seen in table \ref{tab:worksbank}. This sentence contains two ambiguous words: \emph{work} can mean to labor or to function, \emph{bank} can be the river bank and the institution. We can see that our model correctly is able to determine that the probability that \emph{he works} as in earn his living is much more probable than \emph{he works} in the sense that a machine is functioning. The model has more problems with disambiguating between the different senses of \emph{bank} though, which can be traced to the sparseness of these expressions in the dataset.


\begin{table}[htbp]
\centering
\begin{tabular}{lll}
\multicolumn{3}{l}{``He works at the bank''}\\
\textbf{GF parser} & \textbf{Rerank} & \textbf{Interpretation}\\
26.398 & 16.542 & he \emph{earns his living} at the \emph{bank institution}  \\
29.458 & 16.802 & he \emph{earns his living} at the \emph{river bank} \\
26.398 & 45.383 & he \emph{functions} at the \emph{bank institution}\\
29.458 & 37.562 & he \emph{functions} at the \emph{river bank}   
\end{tabular}
\caption{The complete phrases that the GF parser generate for the phrase ``he works at the bank''. The numbers are the negative log probabilities from the GF parser and from our reranking. We can see that our model successfully assigns a high probability to \emph{earn a living} as opposed to \emph{is functioning} at a bank.}
\label{tab:worksbank}
\end{table}

\begin{table}[htbp]
\centering
\begin{tabular}{ll}
\multicolumn{2}{l}{\bf Test sentences}\\
GF & 21 \\
Rerank & 36 \\
Total \# of sentences & 47   
\end{tabular}
\caption{Total number of successful rankings of trees by the GF parser and our reranking model. }
\label{tab:gfparser}
\end{table}


\section{Word Sense Disambiguation I: Trainomatic}

In addition to the qualitative evaluation described in the previous section we also wanted to do a quantitative evaluation. In order to do so it is vital to define a clear testing criteria with a clear right or wrong answer. Here, the word sense disambiguation is a suitable subtask. It is important to point out that this is only one part of tree disambiguation.

The first dataset we utilized was the Trainomatic dataset \citep{pasini2017trainomatic}, a large automatically tagged set of sentences where one word has been tagged with its wordnet synset. This gives us a large testing dataset and the results can be seen in table \ref{tab:gf_trainomatic} and \ref{tab:wordnet_trainomatic}. The evaluation is run on English language data and our model is trained with the tree different datasets described in section \ref{section:training}. As described in section \ref{sec:models}, we defined five different variations of our model:

\begin{enumerate}
    \item Unigram.
    \item Bigram with dependency relationship label information.
    \item Bigram without dependency relationship label information.
    \item Interpolation with dependency relationship label information.
    \item Interpolation without dependency relationship label information.
\end{enumerate}
each of which was trained on the tree different language constellations defined in section \ref{section:training}. The results for the models using the GF dictionery is shown in table \ref{tab:gf_trainomatic}. We see that the precision is very high across the board, which can be explained by that the ambiguity of the English dictionary is very low (see table \ref{tab:dict_stats}). This means that the sentences we are able to assign a abstract function to will generally be correct since the majority of these are unambiguous. In table \ref{tab:wordnet_trainomatic}, the results from using the Wordnet dictionary is used. We see that we get a performance boost for using the clustered Wordnet. 
\begin{table}[]
    \centering\begin{tabular}{llccc}
\textbf{Model}  & \textbf{Training Data}        & \textbf{Precision} & \textbf{Recall} & \textbf{F1}     \\
\hline
\multirow{3}{*}{ GF Unigram }             & 5 languages  & 95.3      & 47.0   & 63.0   \\
                                           & No English   & 95.3      & 47.0   & 63.0   \\
                                           & Only English & 95.4      & 47.1   & 63.1   \\
\hline
\multirow{3}{*}{ GF Interpolation Deprel } & 5 languages  & 94.9      & 46.8   & 62.7   \\
                                           & No English   & 89.1      & 44.0   & 58.9   \\
                                           & Only English & 94.3      & 46.5   & 62.3   \\
\hline
\multirow{3}{*}{ GF Interpolation }        & 5 languages  & 95.6      & 47.2   & 63.2   \\
                                           & No English   & 95.1      & 47.0   & 62.9   \\
                                           & Only English & 95.1      & 46.9   & 62.8   \\
\hline
\multirow{3}{*}{ GF Bigram Deprel }       & 5 languages  & 94.9      & 46.9   & 62.8   \\
                                           & No English   & 89.2      & 44.0   & 59.0   \\
                                           & Only English & 94.1      & 46.5   & 62.2   \\
\hline
\multirow{3}{*}{ Gf Bigram }               & 5 languages  & 95.7      & 47.2   & 63.2   \\
                                           & No English   & 95.3      & 47.0   & 63.0   \\
                                           & Only English & 95.1      & 46.9   & 62.8   \\
\hline
\multirow{1}{*}{ Random Baseline } & & 95.4 & 47.1 & 63.1 \\
\end{tabular}

    \caption{Results for models using the GF dictionary on the Trainomatic evaluation set.}
    \label{tab:gf_trainomatic}
\end{table}

\begin{table}[htbp]
\centering
\begin{tabular}{llccc}
\textbf{Model} & \textbf{Training Data} & \textbf{P} & \textbf{R} & \textbf{F1} \\
\hline
\multirow{3}{*}{ Wordnet Unigram }                & 5 languages  & 30.2      & 24.7   & 27.2    \\
                                                  & No English   & 32.8      & 27.1   & 29.7    \\
                                                  & Only English & 27.5      & 22.9   & 25.0    \\
\hline
\multirow{3}{*}{ Wordnet Interpolation Deprel }   & 5 languages  & 32.7      & 27.1   & 29.6    \\
                                                  & No English   & 29.2      & 24.0   & 26.3    \\
                                                  & Only English & 34.4      & 28.1   & 30.9    \\
\hline
\multirow{3}{*}{ Wordnet Interpolation  }         & 5 languages  & 31.2      & 25.8   & 28.3    \\
                                                  & No English   & 32.4      & 26.7   & 29.3    \\
                                                  & Only English & 32.8      & 26.6   & 29.4    \\
\hline
\multirow{3}{*}{ Wordnet Bigram Deprel }          & 5 languages  & 32.8      & 27.1   & 29.7    \\
                                                  & No English   & 30.0      & 24.7   & 27.1    \\
                                                  & Only English & 34.8      & 28.2   & 31.1    \\
\hline
\multirow{3}{*}{ Wordnet Bigram }                 & 5 languages  & 33.1      & 27.3   & 29.9    \\
                                                  & No English   & 31.9      & 26.0   & 28.7    \\
                                                  & Only English & 34.7      & 28.2   & 31.1    \\
\hline
\multirow{3}{*}{ Clustered Interpolation Deprel } & 5 languages  & 48.4      & 43.2   & 45.7    \\
                                                  & No English   & 46.6      & 41.6   & 44.0    \\
                                                  & Only English & 49.0      & 44.2   & 46.4    \\
\hline
\multirow{3}{*}{ Clustered Interpolation }        & 5 languages  & 48.2      & 43.0   & 45.5    \\
                                                  & No English   & 49.1      & 43.9   & 46.3    \\
                                                  & Only English & 48.6      & 43.8   & 46.0    \\
\hline
\multirow{3}{*}{ Clustered Bigram Deprel
 }       & 5 languages  & 49.2      & 43.9   & 46.4    \\
                                                  & No English   & 47.3      & 42.6   & 44.8    \\
                                                  & Only English & 49.9      & 44.6   & 47.1    \\
\hline
\multirow{3}{*}{ Clustered Bigram }               & 5 languages  & 49.2      & 43.9   & 46.4    \\
                                                  & No English   & 48.8      & 43.5   & 46.0    \\
                                                  & Only English & 49.3      & 44.4   & 46.7    \\
\hline
\multirow{1}{*}{ Random Baseline }               &              & 33.0      & 27.3   & 29.9    \\
\end{tabular}

    \caption{Results for models using the Wordnet dictionary on the Trainomatic evaluation set.}
    \label{tab:wordnet_trainomatic}
\end{table}

%% SUPER CLUSTER
% \begin{table}[]
%     \centering
%     \begin{tabular}{llccc}
% \textbf{Model} & \textbf{Training Data} & \textbf{P} & \textbf{R} & \textbf{F1} \\
% \multirow{3}{*}{ Interpolation Deprel } & 5 languages & 2.0 & 1.9 & 2.0 \\
% \hline
% & No English & 2.2 & 2.0 & 2.1 \\
% & Only English & 2.1 & 1.9 & 2.0 \\
% \hline
% \multirow{3}{*}{ Interpolation } & 5 languages & 2.1 & 2.0 & 2.0 \\
% & No English & 2.1 & 2.0 & 2.1 \\
% & Only English & 2.2 & 2.0 & 2.1 \\
% \hline
% \multirow{3}{*}{ Bigram Deprel } & 5 languages & 2.1 & 2.0 & 2.1 \\
% & No English & 2.0 & 1.9 & 1.9 \\
% & Only English & 2.1 & 2.0 & 2.0 \\
% \hline
% \multirow{3}{*}{ Bigram } & 5 languages & 2.0 & 1.9 & 2.0 \\
% & No English & 2.2 & 2.0 & 2.1 \\
% & Only English & 2.0 & 1.9 & 1.9 \\
% \hline
% \multirow{1}{*}{ Random Baseline }  &  & 33.0 & 27.3   & 29.9    \\
% \end{tabular} 
%     \caption{Results for models using the super clustered wordnet dictionary on the Trainomatic evaluation set.}
%     \label{tab:superclust_trainomatic}
% \end{table}

\section{Word Sense Disambiguation II: Semeval}
The second word sense disambiguation dataset is from the Semeval 2015 task 13 \citep{moro2015semeval}. Titled \emph{Multilingual All-Words Sense Disambiguation and Entity Linking}, the task focus both on word sense disambiguation and entity linking, while our method only performs word sense disambiguation. We used the five different models from section \ref{sec:models} and annotated the given sentences with our predicted Wordnet senses. The authors of the task provided a scorer which we used, and got the results given in table \ref{tab:wordnet_semeval} for normal Wordnet and in table \ref{tab:clust_semeval} for the merged Wordnet. Our results are compareble to the \textbf{TeamUFAL} system described in the Semeval task description \citep{moro2015semeval}. TeamUFAL is a unsupervised method using Wikipedia and Wordnet senses to disambiguate senses.
\begin{table}[]
    \centering
\begin{tabular}{llccc}
\textbf{Model}  & \textbf{Training Data} & \textbf{P}     & \textbf{R}    & \textbf{F1}  \\
\hline
\multirow{3}{*}{ Wordnet Bigram Deprel
 }       & 5 languages   & 47.7  & 43.1  & 45.3    \\
                                                & No English    & 44.9  & 40.5  & 42.6    \\
                                                & Only English  & 44.5  & 40.2  & 42.2    \\
\hline
\multirow{3}{*}{ Wordnet Bigram }               & 5 languages   & 46.9  & 42.3  & 44.5    \\
                                                & No English    & 45.3  & 40.9  & 43.0    \\
                                                & Only English  & 44.2  & 39.9  & 42.0    \\
\hline
\multirow{3}{*}{ Wordnet Interpolation Deprel } & 5 languages   & 46.4  & 41.9  & 44.0    \\
                                                & No English    & 42.6  & 38.5  & 40.4    \\
                                                & Only English  & 44.4  & 40.1  & 42.1    \\
\hline
\multirow{3}{*}{ Wordnet Interpolation
 }       & 5 languages   & 44.3  & 40.0  & 42.1    \\
                                                & No English    & 43.1  & 38.9  & 40.9    \\
                                                & Only English  & 44.2  & 39.9  & 42.0    \\
\hline
\multirow{3}{*}{ Wordnet Unigram }              & 5 languages   & 45.0  & 40.6  & 42.7    \\
                                                & No English    & 46.7  & 42.1  & 44.3    \\
                                                & Only English  & 40.3  & 36.3  & 38.2    \\
\end{tabular}
    \caption{Results for models using the Wordnet dictionary on the Semeval 2015 test data set.}
    \label{tab:wordnet_semeval}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{ll ccc}
\textbf{Model} & \textbf{Training Data} & \textbf{P} & \textbf{R} & \textbf{F1} \\
\hline
\multirow{3}{*}{ Clustered Bigram Deprel } & 5 languages & 44.7 & 40.3 & 42.4 \\
& No English & 43.8 & 39.5 & 41.5 \\
& Only English & 46.0 & 41.5 & 43.7 \\
\hline
\multirow{3}{*}{ Clustered Bigram } & 5 languages & 45.8 & 41.3 & 43.4 \\
& No English & 46.4 & 41.9 & 44.0 \\
& Only English & 47.2 & 42.6 & 44.7 \\
\hline
\multirow{3}{*}{ Clustered Interpolation Deprel } & 5 languages & 44.2 & 39.8 & 41.9 \\
& No English & 44.2 & 39.8 & 41.9 \\
& Only English & 46.7 & 42.1 & 44.3 \\
\hline
\multirow{3}{*}{ Clustered Interpolation } & 5 languages & 44.8 & 40.4 & 42.5 \\
& No English & 46.9 & 42.3 & 44.5 \\
& Only English & 45.8 & 41.4 & 43.5 \\
\hline
\multirow{3}{*}{ Clustered Unigram } & 5 languages & 44.7 & 40.3 & 42.4 \\
& No English & 46.1 & 41.6 & 43.8 \\
& Only English & 44.8 & 40.4 & 42.5 \\
\end{tabular}

    \caption{Results for models using the Clustered Wordnet dictionary on the Semeval 2015 test data set.}
    \label{tab:clust_semeval}
\end{table}
